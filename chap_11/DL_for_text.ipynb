{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51eaf9c4",
   "metadata": {},
   "source": [
    "# Deep Learning for Text\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6d8dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating System\n",
    "import os, shutil, pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Math Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# TensorFlow Datasets\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fff45f",
   "metadata": {},
   "source": [
    "#### Preparing Text Data\n",
    "\n",
    "##### Vocabulary Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79b1c76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor text in dataset:\\n    text = standardize(text)\\n    tokens = tokenize(text)\\n    for toke in tokens:\\n        if token not in vocabulary:\\n            vocabulary[token] = len(vocabulary)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For instance:\n",
    "\n",
    "vocabulary = {}\n",
    "'''\n",
    "for text in dataset:\n",
    "    text = standardize(text)\n",
    "    tokens = tokenize(text)\n",
    "    for toke in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = len(vocabulary)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8a39017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switching integer into a vector encoding as follow:\n",
    "\n",
    "def one_hot_encode_token(token):\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a6dea",
   "metadata": {},
   "source": [
    "##### Using The TextVectorization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62deffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72f5472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and the\",\n",
    "    \"A poppy blooms.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38bdb23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52a8b449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a85508de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cebbf5",
   "metadata": {},
   "source": [
    "##### TextVectorization layer as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "451e0619",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09329ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Function on TextVectorization as follow:\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "685bfc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec4613",
   "metadata": {},
   "source": [
    "##### Displaying The Vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72157011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb4912dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Encode procedure:\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "460a20bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "# Decode Procedure:\n",
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429e8e4",
   "metadata": {},
   "source": [
    "### Two (2) Approaches for Representing Group of Words: Sets & Sequences\n",
    "\n",
    "##### preparing the IDBM movie reviews data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57fa2cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  5336k      0  0:00:15  0:00:14  0:00:01 6567k.7M    0     0  5191k      0  0:00:15  0:00:13  0:00:02 6269k   0     0  5381k      0  0:00:15  0:00:15 --:--:-- 6827k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d8add1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting subdirectory as follow\n",
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f55f57f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
     ]
    }
   ],
   "source": [
    "# Zooming in sample\n",
    "!cat aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6ef788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting validation set at 20% as follow:\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c5a766c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bf8221",
   "metadata": {},
   "source": [
    "##### Displaying the shapes and dtypes of the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b041fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b\"Escaping the life of being pimped by her father(..and the speakeasy waitressing)who dies in an explosion, Lily Powers(Barbara Stanwyck, who is simply ravishing)sluts her way through the branches inside a bank business in big city Gotham. When a possessive lover murders who was supposed to be his next father-in-law(and Lily's new lover), the sky's the limit for Lily as she has written down her various relationships in a diary and subtlety makes it known the papers will receive it if certain pay doesn't come into her hands. Newly appointed president to the bank, Courtland Trenholm(George Brent), sends Lily to Paris instead of forking over lots of dough, but soon finds himself madly in love after various encounters with her in the City of Love. This makes Lily's mouth water as now she'll have reached the pedestal of success seducing a man of wealth and prestige bring riches her way. Though, circumstances ensue which will bring her to make a decision that threatens her successful way of achieving those riches..Trenholm, now her husband, is being indicted with jail certain and has lost the bank. He needs money Lily now has in her possession or he'll have absolutely nothing.<br /><br />Stanwyck is the whole movie despite that usual Warner Brothers polish. Being set in the pre-code era gives the filmmakers the chance to elaborate on taboo subjects such as a woman using sex to achieve success and how that can lead to tragedy. Good direction from Alfred E Green shows through subtlety hints in different mannerisms and speech through good acting from the seductive performance of Stanwyck how to stage something without actually showing the explicit act. Obviously the film shows that money isn't everything and all that jazz as love comes into the heart of Lily's dead heart. That ending having Lily achieve the miraculous metamorphosis into someone in love didn't ring true to me. She's spent all this time to get to that platform only to fall for a man who was essentially no different than others she had used before him.\", shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978eee70",
   "metadata": {},
   "source": [
    "#### Processing Words as a Set: The bag-of-words Approach\n",
    "\n",
    "##### Preprocessing datasets with a TextVectorization layer as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fb00136",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0f1799",
   "metadata": {},
   "source": [
    "##### Inspecting the output of our binary unigram dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17a7d717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e5cacf",
   "metadata": {},
   "source": [
    "##### model-building utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5070ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model function as follow:\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4eb68c",
   "metadata": {},
   "source": [
    "##### Training and testing the binary unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4cea25cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.4271 - accuracy: 0.8163 - val_loss: 0.2795 - val_accuracy: 0.8910\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 929us/step - loss: 0.2885 - accuracy: 0.8950 - val_loss: 0.2685 - val_accuracy: 0.8980\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 857us/step - loss: 0.2578 - accuracy: 0.9092 - val_loss: 0.2717 - val_accuracy: 0.8976\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 786us/step - loss: 0.2419 - accuracy: 0.9179 - val_loss: 0.2855 - val_accuracy: 0.8978\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 810us/step - loss: 0.2280 - accuracy: 0.9221 - val_loss: 0.2888 - val_accuracy: 0.9000\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 0s 795us/step - loss: 0.2232 - accuracy: 0.9254 - val_loss: 0.3004 - val_accuracy: 0.8974\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 0s 790us/step - loss: 0.2219 - accuracy: 0.9286 - val_loss: 0.3058 - val_accuracy: 0.8992\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 803us/step - loss: 0.2159 - accuracy: 0.9300 - val_loss: 0.3125 - val_accuracy: 0.8968\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 0s 799us/step - loss: 0.2175 - accuracy: 0.9302 - val_loss: 0.3161 - val_accuracy: 0.8970\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 0s 787us/step - loss: 0.2094 - accuracy: 0.9327 - val_loss: 0.3290 - val_accuracy: 0.8938\n",
      "782/782 [==============================] - 1s 914us/step - loss: 0.2903 - accuracy: 0.8876\n",
      "Test acc: 0.888\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aeb22c",
   "metadata": {},
   "source": [
    "#### Bigrams with Binary Encoding\n",
    "\n",
    "##### Configuring the TextVectorization layer to return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9441219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90659b",
   "metadata": {},
   "source": [
    "##### Training and testing the binary bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5d400c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3808 - accuracy: 0.8418 - val_loss: 0.2467 - val_accuracy: 0.9058\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2414 - accuracy: 0.9147 - val_loss: 0.2535 - val_accuracy: 0.9102\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 862us/step - loss: 0.2066 - accuracy: 0.9314 - val_loss: 0.2576 - val_accuracy: 0.9102\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 0s 795us/step - loss: 0.1917 - accuracy: 0.9425 - val_loss: 0.2749 - val_accuracy: 0.9086\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 0s 796us/step - loss: 0.1769 - accuracy: 0.9448 - val_loss: 0.2861 - val_accuracy: 0.9098\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 844us/step - loss: 0.1840 - accuracy: 0.9485 - val_loss: 0.2911 - val_accuracy: 0.9090\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 879us/step - loss: 0.1786 - accuracy: 0.9499 - val_loss: 0.3030 - val_accuracy: 0.9064\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 853us/step - loss: 0.1793 - accuracy: 0.9516 - val_loss: 0.3144 - val_accuracy: 0.9036\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 869us/step - loss: 0.1818 - accuracy: 0.9517 - val_loss: 0.3166 - val_accuracy: 0.9014\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 800us/step - loss: 0.1826 - accuracy: 0.9531 - val_loss: 0.3229 - val_accuracy: 0.9022\n",
      "782/782 [==============================] - 1s 931us/step - loss: 0.2659 - accuracy: 0.8966\n",
      "Test acc: 0.897\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6721343",
   "metadata": {},
   "source": [
    "#### Bigrams with TF-IDF Encoding\n",
    "\n",
    "##### Configuring the TextVectorization layer to return token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73d9d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c968f655",
   "metadata": {},
   "source": [
    "##### Configuring TextVectorization to return TF-IDF-weighted outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97b4dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"tf_idf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34e3eb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef tfidf(term, document, dataset):\\n    term_freq = document.count(term)\\n    doc_freq = math.log(sum(doc.count(term) for doc in dataset) + 1)\\n    return term_freq / doc_freq\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def tfidf(term, document, dataset):\n",
    "    term_freq = document.count(term)\n",
    "    doc_freq = math.log(sum(doc.count(term) for doc in dataset) + 1)\n",
    "    return term_freq / doc_freq\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec6b01b",
   "metadata": {},
   "source": [
    "##### Training and testing the TF-IDF bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7c4e05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.5639 - accuracy: 0.7148 - val_loss: 0.3138 - val_accuracy: 0.8856\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 915us/step - loss: 0.4268 - accuracy: 0.8069 - val_loss: 0.3192 - val_accuracy: 0.8338\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 882us/step - loss: 0.3745 - accuracy: 0.8393 - val_loss: 0.3183 - val_accuracy: 0.8666\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.3539 - accuracy: 0.8512 - val_loss: 0.2907 - val_accuracy: 0.8932\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 890us/step - loss: 0.3196 - accuracy: 0.8641 - val_loss: 0.3167 - val_accuracy: 0.8892\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 828us/step - loss: 0.2942 - accuracy: 0.8713 - val_loss: 0.3100 - val_accuracy: 0.8840\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 838us/step - loss: 0.2796 - accuracy: 0.8766 - val_loss: 0.3254 - val_accuracy: 0.8836\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 852us/step - loss: 0.2801 - accuracy: 0.8804 - val_loss: 0.3320 - val_accuracy: 0.8600\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 839us/step - loss: 0.2755 - accuracy: 0.8774 - val_loss: 0.3304 - val_accuracy: 0.8804\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 898us/step - loss: 0.2641 - accuracy: 0.8820 - val_loss: 0.3431 - val_accuracy: 0.8742\n",
      "782/782 [==============================] - 1s 983us/step - loss: 0.3202 - accuracy: 0.8774\n",
      "Test acc: 0.877\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c21c96a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "outputs = model(processed_inputs)\n",
    "inference_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb470a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.02 percent positive\n"
     ]
    }
   ],
   "source": [
    "raw_text_data = tf.convert_to_tensor([\n",
    "    [\"That was an excellent movie, I loved it.\"],\n",
    "])\n",
    "predictions = inference_model(raw_text_data)\n",
    "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f10bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
